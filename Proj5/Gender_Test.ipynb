{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from _helper_basics_ import *\n",
    "from _helper_gender_ import *\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 2.0.8\n",
      "tensorflow 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "## To get helper functions from another folder\n",
    "# sys.path.insert(0, '../') # if _helper_basics_ is in previous folder\n",
    "now_i_am_at = 'home' # home dso test\n",
    "if now_i_am_at=='home': sys.path.insert(0, 'E:/Leonard HDD/Dropbox/DSO/Tasks/')\n",
    "elif now_i_am_at=='dso': sys.path.insert(0, 'D:/Dropbox/DSO/Tasks')\n",
    "\n",
    "from _helper_basics_ import *\n",
    "from _helper_gender_ import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('keras',      keras.__version__)\n",
    "print('tensorflow', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfft:256 hop_length:85 \n",
      "num_freq:129 num_time:48 num_channel:1\n"
     ]
    }
   ],
   "source": [
    "yf = np.asarray([0,1])\n",
    "ym = np.asarray([1,0])\n",
    "\n",
    "\n",
    "nfft = 256   # sr=8000\n",
    "# nfft = 512   # sr=16000\n",
    "hop_length = nfft//3 # 256 = 30ms\n",
    "num_freq,num_time,num_channel    = nfft//2+1,48,1 # 0.5 seconds speech is about 48 time_samples\n",
    "print('nfft:{}'.format(nfft),'hop_length:{}'.format(hop_length),\n",
    "      '\\nnum_freq:{}'.format(num_freq),'num_time:{}'.format(num_time),'num_channel:{}'.format(num_channel))\n",
    "Spect_Det = [nfft, hop_length, num_freq,num_time,num_channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwd E:\\Leonard HDD\\Dropbox\\Meetup\\Deep Learning Developer Course\\Personal2_Gender_Audio_Classification\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AMI',\n",
       " 'BABEL_CORPUS',\n",
       " 'Gender',\n",
       " 'Google_Speech_Commands',\n",
       " 'MUSAN',\n",
       " 'Reverb_1',\n",
       " 'Reverb_2',\n",
       " 'RIRS_NOISES',\n",
       " 'Speech MNIST',\n",
       " 'Speeches',\n",
       " 'Switchboard',\n",
       " 'TIMIT',\n",
       " 'Voice_Converision',\n",
       " 'z.samples']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "print('pwd', pwd)\n",
    "\n",
    "Dataset_dir = os.path.join(pwd,'..','..','..','Speech Audio Text','3) Dataset')\n",
    "get_immediate_subdirectories(Dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data_1\n",
    "    Google Speech Command Dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data_2\n",
    "    Downloaded in https://datashare.is.ed.ac.uk/handle/10283/2211\n",
    "    Phrases spoken by different people\n",
    "    But are not aligned as different speakers have different styles and length\n",
    "    \n",
    "    Currently v1_FCN is trained on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_list: ['SF1', 'SF2', 'SF3', 'SM1', 'SM2', 'TF1', 'TF2', 'TM1', 'TM2', 'TM3']\n",
      "dir_list_F ['SF1', 'SF2', 'SF3', 'TF1', 'TF2']\n",
      "dir_list_M ['SM1', 'SM2', 'TM1', 'TM2', 'TM3']\n",
      "trg_path_list 972\n",
      "val_path_list 324\n",
      "test_path_list 324\n"
     ]
    }
   ],
   "source": [
    "Data_2_dir = os.path.join(Dataset_dir,'Voice_Converision', 'Data_2_Datashare','vcc2016_training')\n",
    "dir_list = get_immediate_subdirectories(Data_2_dir) \n",
    "print('dir_list:',dir_list)\n",
    "\n",
    "pwd = os.getcwd() # 'D:\\\\Dropbox\\\\Meetup\\\\Deep Learning Developer Course\\\\Personal2_Gender_Audio_Classification'\n",
    "Data_tdt = os.path.join(pwd,'Data_tdt')\n",
    "trg_dir = os.path.join(Data_tdt,'train')\n",
    "\n",
    "## Male Female\n",
    "dir_list_F,dir_list_M = [],[]\n",
    "for i in dir_list:\n",
    "    if 'F' in i: dir_list_F.append(i)\n",
    "    elif 'M' in i: dir_list_M.append(i)\n",
    "print('dir_list_F',dir_list_F)\n",
    "print('dir_list_M',dir_list_M)\n",
    "\n",
    "## ListW\n",
    "TRG_list,VAL_list, TEST_list = [],[],[]\n",
    "for i in range(len(dir_list)//2):\n",
    "    if i==3:\n",
    "        VAL_list.append(dir_list_F[i])\n",
    "        VAL_list.append(dir_list_M[i])\n",
    "    elif i==4:\n",
    "        TEST_list.append(dir_list_F[i])\n",
    "        TEST_list.append(dir_list_M[i])\n",
    "    else:\n",
    "        TRG_list.append(dir_list_F[i])\n",
    "        TRG_list.append(dir_list_M[i])\n",
    "trg_path_list,trg_list = data_2_tdt(TRG_list, Data_2_dir)\n",
    "print('trg_path_list', len(trg_path_list))\n",
    "val_path_list,val_list = data_2_tdt(VAL_list, Data_2_dir)\n",
    "print('val_path_list', len(val_path_list))\n",
    "test_path_list,test_list = data_2_tdt(TEST_list, Data_2_dir)\n",
    "print('test_path_list', len(test_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train (9947, 2)\n",
      "X_train (9947, 129, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "Y_train = dump_load_pickle(os.path.join(trg_dir,'Y_train'), 'load')\n",
    "X_train = dump_load_pickle(os.path.join(trg_dir,'X_train'), 'load')\n",
    "X_train = X_train.reshape(len(X_train), num_freq,num_time,num_channel)\n",
    "print('Y_train', Y_train.shape)\n",
    "print('X_train', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_wav 972\n",
      "X_train_lps 972\n"
     ]
    }
   ],
   "source": [
    "X_train_wav = dump_load_pickle(os.path.join(pwd,'Data_tdt','train','X_train_wav'), 'load')\n",
    "X_train_lps = dump_load_pickle(os.path.join(pwd,'Data_tdt','train','X_train_lps'), 'load')\n",
    "print('X_train_wav', len(X_train_wav))\n",
    "print('X_train_lps', len(X_train_lps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male_num 5046\n",
      "female_num 4901\n"
     ]
    }
   ],
   "source": [
    "male_num,female_num = 0,0\n",
    "for i in Y_train:\n",
    "    if np.any(i==ym): male_num += 1\n",
    "    else: female_num += 1\n",
    "        \n",
    "print('male_num',male_num)\n",
    "print('female_num',female_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data_3\n",
    "    Timit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TIMIT_Gender_Folder = os.path.join(pwd, 'TIMIT_Gender')\n",
    "\n",
    "Data_3_dir = os.path.join(Dataset_dir,'TIMIT')\n",
    "trg_dir  = os.path.join(Data_3_dir, 'timit_train')\n",
    "trg_path_list = glob.glob( os.path.join(trg_dir, '*.wav') )\n",
    "trg_list = []\n",
    "for i in trg_path_list:\n",
    "    trg_list.append( i[len(trg_dir)+1:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (40739, 129, 48, 1)\n",
      "Y_train (40739, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train  = dump_load_pickle(os.path.join(TIMIT_Gender_Folder,'train','X_train'), 'load')\n",
    "Y_train  = dump_load_pickle(os.path.join(TIMIT_Gender_Folder,'train','Y_train'), 'load')\n",
    "print('X_train',X_train.shape)\n",
    "print('Y_train',Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_wav 4619\n",
      "X_train_lps 4619\n"
     ]
    }
   ],
   "source": [
    "X_train_wav = dump_load_pickle(os.path.join(TIMIT_Gender_Folder,'train','X_train_wav'), 'load')\n",
    "X_train_lps = dump_load_pickle(os.path.join(TIMIT_Gender_Folder,'train','X_train_lps'), 'load')\n",
    "print('X_train_wav',len(X_train_wav))\n",
    "print('X_train_lps',len(X_train_lps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1 Simple ConvNet + Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from path : v1_FCN/Logs/v1_FCN_model_v1_2.json\n",
      "Loaded weights         : v1_FCN/Logs/v1_FCN_weights_v1_2.h5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 129, 48, 1)        0         \n",
      "_________________________________________________________________\n",
      "0_Conv2D (Conv2D)            (None, 129, 48, 64)       640       \n",
      "_________________________________________________________________\n",
      "1_Conv2D (Conv2D)            (None, 129, 48, 64)       36928     \n",
      "_________________________________________________________________\n",
      "1_Pool (MaxPooling2D)        (None, 64, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "2_Conv2D (Conv2D)            (None, 64, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "GlobalPool (GlobalAveragePoo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "3_Dense (Dense)              (None, 200)               13000     \n",
      "_________________________________________________________________\n",
      "4_Dense (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "5_Output (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 107,798\n",
      "Trainable params: 107,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Archi_dir = \"v1_FCN/\"\n",
    "Weights_path = Archi_dir+\"Logs/\"\n",
    "Ckpt_Mod_Weights_fold = Archi_dir+\"Checkpoint_Model_Weights/\"\n",
    "\n",
    "model_path = Weights_path+Archi_dir[:-1]+\"_model_v1_1.json\"\n",
    "weights_path = Weights_path+Archi_dir[:-1]+\"_weights_v1_1.h5\"\n",
    "# weights_path = Ckpt_Mod_Weights_fold+\"v1_1/\"+\"weights_v1_1_Epoch-0282_L-0.01.hdf5\"\n",
    "\n",
    "model_path = Weights_path+Archi_dir[:-1]+\"_model_v1_2.json\"\n",
    "weights_path = Weights_path+Archi_dir[:-1]+\"_weights_v1_2.h5\"\n",
    "# weights_path = Ckpt_Mod_Weights_fold+\"v1_1/\"+\"weights_v1_1_Epoch-0282_L-0.01.hdf5\"\n",
    "\n",
    "v1 = load_model(model_path, 'path', weights_path)\n",
    "v1.summary()\n",
    "\n",
    "# adam_opt = Adam(lr=1e-3)\n",
    "sgd_opt = SGD(lr=1e-3, momentum=0.9)\n",
    "v1.compile(loss=categorical_crossentropy ,optimizer=sgd_opt, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_train = v1.evaluate(X_train, Y_train, batch_size=256)\n",
    "# score_val = v1.evaluate(X_val, Y_val, batch_size=256)\n",
    "# score_test = v1.evaluate(X_test, Y_test, batch_size=256)\n",
    "\n",
    "print('score_train',score_train)\n",
    "# print('score_val',score_val)\n",
    "# print('score_test',score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender-Split Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_num=11482, male_num=29257, female_num=11482, idx=40738\n",
      "male_num 29257\n",
      "female_num 11482\n"
     ]
    }
   ],
   "source": [
    "X_train_male,Y_train_male,X_train_female,Y_train_female = gender_split(X_train,Y_train)\n",
    "# X_val_male,Y_val_male,X_val_female,Y_val_female = gender_split(X_val,Y_val)\n",
    "# X_test_male,Y_test_male,X_test_female,Y_test_female = gender_split(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29257/29257 [==============================] - 112s   \n",
      "11482/11482 [==============================] - 47s    \n",
      "score_male [0.97660668753475877, 0.076767952967501718]\n",
      "score_female [0.1509068370296282, 0.9999129048956954]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "print('Predicting Male...');   score_male   = v1.evaluate(X_train_male,   Y_train_male,   batch_size=batch_size)\n",
    "print('Predicting Female...'); score_female = v1.evaluate(X_train_female, Y_train_female, batch_size=batch_size)\n",
    "# score_total = v1.evaluate(X_train, Y_train, batch_size=256)\n",
    "\n",
    "print('score_male',score_male)\n",
    "print('score_female',score_female)\n",
    "# print('score_total',score_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Female : 95.87957859039307%\n",
      "Prediction:  [ 0.04120423  0.95879579]\n",
      "Truth     :  [0 1]\n",
      "Predicted Male : 42.107248306274414%\n",
      "Prediction:  [ 0.42107248  0.57892752]\n",
      "Truth     :  [1 0]\n"
     ]
    }
   ],
   "source": [
    "## Random Female Prediction\n",
    "idx = np.random.randint(0,len(X_train_female))\n",
    "yf = np.asarray([0,1])\n",
    "ym = np.asarray([1,0])\n",
    "## Prediction\n",
    "y_hat_train = v1.predict(X_train_female[idx].reshape(1,num_freq,num_time,num_channel))\n",
    "print('Predicted Female : {}%'.format(np.sum(y_hat_train*yf)*100.) )\n",
    "print('Prediction: ', y_hat_train[0])\n",
    "print('Truth     : ', Y_train_female[idx])\n",
    "\n",
    "## Random Male Prediction\n",
    "idx = np.random.randint(0,len(X_train_male))\n",
    "yf = np.asarray([0,1])\n",
    "ym = np.asarray([1,0])\n",
    "## Prediction\n",
    "y_hat_train = v1.predict(X_train_male[idx].reshape(1,num_freq,num_time,num_channel))\n",
    "print('Predicted Male : {}%'.format(np.sum(y_hat_train*ym)*100.) )\n",
    "print('Prediction: ', y_hat_train[0])\n",
    "print('Truth     : ', Y_train_male[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of softmaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 9.98051526304395%\n",
      "Finished 19.9610305260879%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonard\\Anaconda3\\envs\\virenv_py3_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Leonard\\Anaconda3\\envs\\virenv_py3_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 29.941545789131847%\n",
      "Finished 39.9220610521758%\n",
      "Finished 49.90257631521975%\n",
      "Finished 59.883091578263695%\n",
      "Finished 69.86360684130764%\n",
      "Finished 79.8441221043516%\n",
      "Finished 89.82463736739554%\n",
      "Finished 99.8051526304395%\n",
      "Accuracy: 1535/4619 33.23230136393159%\n"
     ]
    }
   ],
   "source": [
    "# gen_position = 1 # for Data 2\n",
    "gen_position = 4 # for Data 3\n",
    "\n",
    "count_correct = Average_Soft_Pred(v1, gen_position, X_train_lps, trg_list, Spect_Det)\n",
    "print('Accuracy: {}/{} {}%'.format(count_correct,len(trg_list),100.*count_correct/len(trg_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from _helper_basics_ import *\n",
    "from _helper_gender_ import *\n",
    "%reload_ext autoreload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
